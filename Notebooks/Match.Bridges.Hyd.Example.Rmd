---
title: 'Example: Geolocating and Cross-Database Matching of US Hydraulic Bridge Collapses'
author: '[Madeleine Flint](http://www.madeleinemflint.com)'
date: "`r Sys.Date()`"
output:
  html_notebook:
    bibliography: bibliography.bib
    toc: no
  html_document:
    df_print: paged
    toc: no
params:
  eval: no
  mstyle: 'PAR'
---


```{r setup, include=FALSE, eval=params$eval}
library(knitr)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
opts_chunk$set(fig.width=12, fig.height=4, fig.path='Plots/',message=FALSE)
```

```{r directories, include=FALSE, eval=params$eval}
# Hidden: check for consistency with cloned git Repo and for presence of necessary folders and files.
gitRepoName <- "bridge.collapses"
if(grepl(gitRepoName,gsub(paste0("/",gitRepoName),"",getwd()))) warning("Git repository folder name is duplicated, please consider re-cloning.")
if(!grepl(gitRepoName,getwd()))stop("Notebook only functions with working directory set to original repository.")
fold.req <- c("Scripts", "Data")
if (any(!(fold.req %in% list.dirs(path = getwd(), full.names = FALSE, recursive = FALSE)))) stop("Scripts and Data directories and files must be present.")
files.req <- c(paste0(gitRepoName,".Rproj"), "README.md")
if(any(!(files.req %in% list.files()))) stop("Seeming mismatch with directories.")

# check for folders and create analysis folders if missing
fold.req <- c("Plots", "Analysis")
dirsCreate <- fold.req[!(fold.req %in% list.dirs(recursive = FALSE, full.names = FALSE))]
if(length(dirsCreate)) sapply(dirsCreate, function(d) dir.create(d))
fold.req <- c("Processed","Temp","Results","PostProcessed")
dirsCreate <- fold.req[!(fold.req %in% list.dirs(path = "Analysis", recursive = FALSE, full.names = FALSE))]
if(length(dirsCreate)) sapply(dirsCreate, function(d) dir.create(file.path("Analysis",d))) 

# cleanup
rm(fold.req,dirsCreate,gitRepoName,files.req)
```

```{r installLoadPackages, include=FALSE, eval=params$eval}
# Hidden: install and load packages
packages <- c("ggplot2","stringr", "rjson", "knitr", "devtools", 
              "english", "stringdist", "reshape2") 
new.pkg <- packages[!(packages %in% installed.packages())]
if (length(new.pkg)){
    repo <- "http://cran.us.r-project.org"
    install.packages(new.pkg, dependencies = TRUE, repos = repo)
}
invisible(sapply(packages, require, character.only = TRUE))
rm(packages, new.pkg)
```
# Introduction
This R Notebook provides an example of geolocating and augmenting bridge collapse data recorded in the NYSDOT Bridge Failure Database through string-matching to bridges in the 2018 US DOT gis version of the National Bridge Inventory. The analysis described is broken down into the following steps:

1. *Load* and format input data (from .RData or other file formats)
2. Pre-process and *clean* data (using custom dictionaries)
3. Identify potential *features*  within the fields of each collapse entry (using custom dictionaries and mappings)
4. Perform feature *matching* to applicable target data to obtain a set of candidate matches (either sequentially across features or in parallel)
5. Collate and *rank* the candidate matches (based on user-defined hierarchy of match quality)
6. Support supervised match *confirmation* (interactive)

## Example User controls
For the purposes of the example, we can modify the failed bridges we'll look at and the matching method (set in the Notebook parameters).
```{r userExampleControls, include=TRUE, echo = FALSE, eval=params$eval}
EXAMPLE_ID   <- c(31, 175) 
# Matching options
M_STYLE <- params$mstyle # PAR: Runs for each M_TYPE in parallel (i.e., run independently and then combined)
                         # SEQ: Sequential runs (i.e., narrowing down by each match type in order)?
M_TYPE  <- c("road",     # Determines how data should be matched, and supports: 
             "stream",   # 'road', 'route', 'stream', and 'bin' (Bridge Id No). 
             "route",    # Order in vector determines the prioritization in ranking. 
             "bin")
```


```{r advancedUserControls, include=FALSE, eval=params$eval}
# Controls for data use and data storage (memory)
DATA   <- list(MatchData  = "Fail", # not currently modifiable
               TargetData = "NBI",  # not currently modifiable
               Dictionary = c("Stream", "Road", "ITEM5","ITEM43", 
                              "Cardinal", "Relational", "HydraulicCollapse"),
               StdData    = c("STFIPS", "FIPS", "GNIS"), # Standard data sets
               SuppTarget = c("USGS","GRAND"))           # Additional analysis
REDUCE <- list(Fail = c(Example = TRUE,    # only pulls the entry with ID EXAMPLE_ID
                        Fields  = FALSE),  # Extraneous fields deleted?
               NBI  = c(Fields  = TRUE,   
                        States  = TRUE))  # Target data of states not present 
                                          # in match data deleted?


# Controls which parts of Notebook/Analysis are run and data storage (disk)
STEPS <- c("LOAD", "CLEAN", "FEATURE", "MATCH", "RANK", "CONFIRM")
RUN   <- c(FALSE, rep(TRUE, 5))   # Which of the steps above to be run?
SAVE  <- c(rep(FALSE, 6))  # If/when intermediate results to be saved &        
                           # subsequently loaded?
names(RUN)  <- STEPS
names(SAVE) <- STEPS

M_STORE <- TRUE       # List of potential matches for each entry is stored on disk,
                      # rather than held in memory and returned to the notebook.

# Subfunction controls for messages 
VERB <- TRUE # Subfunctions to print intermediate results (e.g., # of matches 
             # completed). Mainly useful for debugging with a small set of MatchData.

# Features      : which types of data will be cleaned and undergo feature detection
# Match         : controls for string matching and sorting
# maxStringDist : for approximate string matching, max distance that will count as 
#                 "approximately" matched, i.e., number of substitutions, deletions, 
#                 insertions, and BLAH.
# capCand*      : when the list of candidate matches (of the potential match targets) 
#                 grows too long the algorithm will assume that none of the candidates are
#                 sufficiently strong, and will return all potential targets.
#   *N          : if number of candidate matches is > capCandN, revoke candidacy
#   *Pct        : if number of candidate matches is > capCandPct*nTargetRows, revoke 
opts <-list(Fields   =list(Fail =c("MAT", "TYPE", "LOCATION", "STREAM", "BIN", "ID"),
                           NBI  =c("ROUTE","LOCATION","ROAD","STREAM", "BIN", "ID")),
            Features =list(Fail =c("COUNTY", "CITY","LOCATION","ROAD","ROUTE", 
                                   "BRIDGE","STREAM"),
                           NBI  =NULL),
            Keys     =list(Fail =list(LOCATION = c("Relational"),
                                      ROAD     = c("Road"),
                                      ROUTE    = c("Rte"),
                                      BRIDGE   = c("Bridge"),
                                      STREAM   = c("Stream", "Trib", "Bridge", "Near")
            )),
            Match=list(maxStringDist =c(road =2, stream =2, route =1, bin =3),
                       capCandN      =c(road =200, stream =100, route =100, bin =200),
                       capCandPct    =c(road =0.7, stream =0.7, route =0.7, bin =0.7)))
```

```{r controlsChecks, include=FALSE, eval=params$eval}
# Hidden: Check that user control inputs are valid.
if (!is.logical(RUN) | !is.logical(SAVE) | length(RUN)!=6 | length(SAVE)!=6){
  stop('RUN and SAVE must be defined as length-6 logicals')
}
if(RUN["MATCH"] & !(M_STYLE %in% c("SEQ", "PAR"))){
  stop('To perform matching, M_STYLE must be defined as SEQ or PAR')
}
if(RUN["MATCH"] & !(all(M_TYPE %in% c("road", "stream","bin","route")))){
  stop('Only matching by road, route, bin, and stream supported.')
}
```

# 1. Load and format raw or minimally pre-processed input data
The `Load.Format.Input.Data` subfunction is used to create match and target dataframes with appropriate data classes and names. Input files in RData, CSV, TAB, XLSX, XLS, or GIS (NBI only) are supported, with packages automatically selected based on the input filename extension:

```{bash, echo = TRUE, comment = ''}
sed -n '46,53p;53q' './Scripts/Load Input/Load.Format.Input.Data.R'
```

```{r loadFormat, include = FALSE, eval=params$eval, error=FALSE, message=FALSE}
fp <- file.path("Data","Input")
inputs<-list(Fail       = c(Fail=file.path(fp,"Fail.Example.RData")),
             NBI        = c(NBI=file.path(fp,"NBI.Example.RData")),
             Dictionary = c(Stream=file.path(fp,"Dictionaries","ls.StreamKeys.RData"),
                            Trib=file.path(fp,"Dictionaries","ls.TribKeys.RData"),
                            Road=file.path(fp,"Dictionaries","ls.RoadKeys.RData"),
                            Rte=file.path(fp,"Dictionaries","ls.RteKeys.RData"),
                            Rail=file.path(fp,"Dictionaries","ls.RailKeys.RData"),
                            Bridge=file.path(fp,"Dictionaries","ls.BridgeKeys.RData"),
                            Cardinal=file.path(fp,"Dictionaries","ls.CardKeys.RData"),
             Relational   = file.path(fp,"Dictionaries","ls.RelationalKeys.RData"),
             Place        = file.path(fp,"Dictionaries","ls.PlaceKeys.RData"),
             Jurisdiction = file.path(fp,"Dictionaries","ls.JurisKeys.RData"),
             County       = file.path(fp,"Dictionaries","ls.CountyKeys.RData"),
             Mat          = file.path(fp,"Dictionaries","ls.MatKeys.RData"),
             Type         = file.path(fp,"Dictionaries","ls.TypeKeys.RData"),
             Sup          = file.path(fp,"Dictionaries","ls.SupKeys.RData"),
             Fail         = file.path(fp,"Dictionaries","ls.Fail.Keys.RData"),
             NBI          = file.path(fp,"Dictionaries","ls.NBI.Keys.RData")),
             Ontologies = c(Fail= file.path(fp,"Ontologies","ls.Fail.Ont.RData"),
                            NBI = file.path(fp,"Ontologies","ls.NBI.Ont.RData"),
                            FailFields = file.path(fp,"Ontologies","ls.Fail.Fields.RData"),
                            NBI.Fields = file.path(fp,"Ontologies","ls.NBI.Fields.RData"),
                            ITEM5B = file.path(fp,"Ontologies","ls.ITEM5B.Ont.RData"),
                            ITEM5BRev = 
                              file.path(fp,"Ontologies","ls.ITEM5B.OntRev.RData"),
                            ITEM43A = file.path(fp,"Ontologies","ls.ITEM43A.Ont.RData"),
                            ITEM43B = file.path(fp,"Ontologies","ls.ITEM43B.Ont.RData"),
                            ITEM43BRev =
                              file.path(fp,"Ontologies","ls.ITEM43B.OntRev.RData"),
                            DOT = file.path(fp,"Ontologies","ls.DOT.Keys.RData")),
             StdData    = c(STFIPS= file.path(fp,"Standard","df.States.RData"),
                            FIPS= file.path(fp,"Standard","df.Counties.RData"),
                            GNIS= file.path(fp,"Standard","df.Cities.RData")),
             SuppTarget = c(USGS="",
                            GRAND=""))
if (RUN["LOAD"]){
source(file.path("Scripts","Load Input","Load.Format.Input.Data.R"))
df.Fail  <- Load.Format.Input.Data(inputs[["Fail"]],
                                     DATA_SET    = "Fail",
                                     REDUCE_SIZE = REDUCE[["Fail"]]["Fields"],
                                     VERB     = VERB)
df.NBI <- Load.Format.Input.Data(inputs[["NBI"]],
                                     DATA_SET    = "NBI",
                                     REDUCE_SIZE = REDUCE[["NBI"]]["Fields"],
                                     VERB     = VERB)
rm(Load.Format.Input.Data)
}
# as there are dozens of lists used as dictionaries, they are loaded into their own environment
# so they don't clutter up the workspace
lenv <- new.env()
invisible(lapply(unlist(inputs[c("Dictionary","Ontologies","StdData")]), load, lenv))
attach(lenv)
invisible(lapply(unlist(inputs[c("Fail","NBI")]), load, .GlobalEnv))
row.names(df.Fail) <- as.character(df.Fail$ID)
df.NBI$OBJECTID_new[duplicated(df.NBI$OBJECTID_new)] <- paste0(df.NBI$OBJECTID_new[duplicated(df.NBI$OBJECTID_new)],".1")
row.names(df.NBI) <- df.NBI$OBJECTID_new
```

The as-loaded example rows in the NYSDOT Bridge Failure Database are:

```{r echo=FALSE}
print(df.Fail[df.Fail$ID %in% EXAMPLE_ID,])
```
Randomly selected examples of rows in the NBI Database are:

```{r echo=FALSE}
print(df.NBI[sample.int(nrow(df.NBI),5),c("STATE_CODE_001","STRUCTURE_NUMBER_008","LOCATION_009","FACILITY_CARRIED_007","ROUTE_NUMBER_005D","FEATURES_DESC_006A")])
```

I analyzed the quality of the entries for each bridge and field to determine the best approach to geolocating the bridges. With only 1948 bridges to study, I anticipated that recall was going to be more important than precision, which was borne out by the high number of bridges with missing fields.
```{r Exploratory, echo=FALSE,message=FALSE}
  # Set up fields of interest, naming convention, color palette, etc. -------
  critFields <- c("BIN","LOCATION","FEAT_UND")
  combFields     <- combn(critFields,2)
  combFieldLevels <- apply(combFields, paste0, MARGIN = 2, collapse = " ")
  combFieldLabels <- apply(substr(combFields,1,3), paste0, MARGIN = 2, collapse = "-")
  
  levels <- c("", critFields, combFieldLevels, paste(critFields, collapse = " "))
  labels <- c("NULL", critFields, combFieldLabels, "THREE")
  colors <- c("gray90", "gold2", "deeppink1", "cyan4", "darkorange3", "green4","darkorchid4","black")
  names(colors) <- labels
  
  colorsCount <- c("gray90", rep("gray65",3), rep("gray40",3), "black")
  names(colorsCount) <- labels
  colorsAll <- c(colors, unique(colorsCount[!(colorsCount %in% colors)]))
  names(colorsAll)[9:10] <- c("TWO", "ONE")
  colorsAll <- colorsAll[c(1,9,10,8,2:4,5:7)]
  colorLabels <- names(colorsAll)
  names(colorsAll) <- colorsAll
  
  source(file.path("./Plotting/getTheme.R"))
  source(file.path("./Plotting/SetupEncoding.R"))
  
  # Get counts of rows with 0, 1, 2, or 3 fields that are non-NA and bind data frame -------
  nonNAs     <- !is.na(df.Fail[,critFields])
  colnames(nonNAs) <- critFields
  df.3 <- data.frame(n = apply(nonNAs, sum, MARGIN = 1))
  df.3[,critFields] <- sapply(1:3, function(j) factor(nonNAs[,j],
                                                           levels =c(TRUE, FALSE),
                                                           labels =c(critFields[j],"")))
  df.3$Comb   <- str_squish(apply(df.3[,critFields], MARGIN = 1, paste, collapse=" "))
  df.3$Comb   <- factor(df.3$Comb, levels = levels, labels = labels)
  df.3$group  <- "ALL"
  df.3$color  <- factor(as.character(df.3$Comb), 
                             levels = labels, 
                             labels = colors)
  df.3$colorP <- factor(as.character(df.3$Comb), 
                             levels = labels, 
                             labels = colorsCount)
  df.2 <- df.3[df.3$n==2,]
  df.2$colorP <- df.2$color
  df.2$group  <- "TWO FIELDS"
  
  df.1 <- df.3[df.3$n==1,]
  df.1$colorP <- df.1$color
  df.1$group  <- "ONE FIELD"
  
  df <- rbind(df.3, df.2, df.1)
  df$group <- factor(df$group, levels = c("ALL","TWO FIELDS", "ONE FIELD"))
  
  # make plot --------
  bar <- ggplot(df, aes(x = group, fill = colorP)) + geom_bar(aes(y=(..count..))) 
  bar + scale_fill_manual(values = colorsAll, labels = colorLabels, name = "Fields", drop = FALSE) +
    geom_text(stat='count', aes(label=..count..), color = "white", size = textP$annotate["PRES"],
              position = position_stack(vjust = 0.5))+
    labs(title = "Fields available for 1948 collapsed bridges", x = "subset of bridges", y = "count") +
    getTheme(outputType="PRES") + theme(axis.ticks.x = element_blank(), axis.ticks.y = element_blank())

  rm(df,df.1,df.2,df.3,combFieldLabels,combFieldLevels,combFields,colors,colorsAll,colorsCount,
     nonNAs,colorLabels,critFields, bar, labels, levels)
```

```{r exampleSetup, include = FALSE, echo = FALSE}
# remove unneeded rows 
if(REDUCE[["Fail"]]["Example"]) df.Fail <- df.Fail[df.Fail$ID %in% EXAMPLE_ID,]
if(REDUCE[["NBI"]]["States"]){
  MatchStates <- unique(df.Fail[,paste0('STFIPS')])
  df.NBI  <- df.NBI[df.NBI$STFIPS %in% MatchStates,]
  rm(MatchStates)
}
```

# 2. Pre-process and clean data 
A subfunction `Clean.Fields` was used to process entries in the collapsed bridge and NBI data, with the types of fields cleaned controlled by `opts.Features`. Pre-processing includes the removal of possible escape characters (e.g., `*` or `&`), extra white space, and leading 0s. Entries were then cleaned of known misspellings and other possibly erroneous information, e.g., transcription errors.

```{r cleanData, include = FALSE, eval=params$eval, message=FALSE}
if (RUN["CLEAN"]){
  fp <- file.path("Analysis","Processed")
  source(file.path("Scripts","Process Input","Clean.Fields.R"))
  
  FieldNames <- sapply(opts[["Fields"]][["Fail"]], function(i) ls.Fail.Ont[[i]][1],
                     USE.NAMES = TRUE)
  df.Fail  <- Clean.Fields(df.Fail, NULL,
                             DATA_SET     = "Fail",
                             FieldNames   = FieldNames,
                             VERB      = VERB)
  
  FieldNames <- sapply(opts[["Fields"]][["NBI"]], function(i) ls.NBI.Ont[[i]][1],
                     USE.NAMES = TRUE)[c("BIN","ROUTE")]
  df.NBI  <- Clean.Fields(df.NBI, NULL,
                              DATA_SET     = "NBI",
                              FieldNames  = FieldNames,
                              VERB      = VERB)

  rm(fp, Clean.Fields, FieldNames)
}
```


```{r echo = FALSE}
print(df.Fail[df.Fail$ID %in% EXAMPLE_ID,
              c("LOC","STREAM_UNDER", "ROUTE_UNDER",
                "BIN_NUM","MATER","TYPE_STRUCT", "SUPPORT", "ITEM43A",
                "ITEM43B")])
```

# 3. Identify features of match and target data
Collapsed bridge "LOCATION" and "FEAT_UND" are searched for features, including county, city, bridge, road, route, and stream names. Other identifying information (e.g., "3.5 miles from") is also identified and isolated. Similar data is obtained from the appropriate fields in the target dataset.

## Methods 
A key aspect of the matching algorithm is the use of dictionaries and ontologies to relate entries between databases. The json ontologies were created by the authors, with extensive reference to existing metadata
schema, databases of abbreviations, and knowledge of bridge engineering. For example,
```{bash, echo = FALSE}
cat './Data/Input/Dictionaries/StreamKeys.json'
```

```{R featureDetection, include = FALSE, eval=params$eval}
if(RUN["FEATURE"]){
  source(file.path("Scripts","Process Input","Find.Features.R"))
  
  Features <- sapply(opts[["Features"]][["Fail"]], function(i) ls.Fail.Ont[[i]][1],
                     USE.NAMES = TRUE)
  Features <- sub("LOCATION","LOC",Features)
  Features["STREAM"] <- "STREAM_UNDER"
  df.Fail  <- Find.Features(df.Fail, 
                              Features,
                              VERBOSE = VERB)
  rm(Find.Features, Features)
}
```

```{r echo=FALSE}
print(df.Fail[df.Fail$ID %in% EXAMPLE_ID,c(30:31,34:35, 46:47,50:56,60:65)])
```

# 4. Structured feature and string matching
Identify NBI bridges that could be linked to failed bridges. Matching style and type of matching controlled by `M_STYLE` and `M_TYPE`. Sequential matching will first search for matches to the first type, and will consider only those possible matching when searching for matches to the second (or later) types. Parallel matching searches for the types separately and then combines the match pools. Currently supported types are `road`, `route`, `stream`, and `bin`. County and city information is automatically searched.
```{r matching, include=TRUE, eval=params$eval, error=FALSE}
if (RUN["MATCH"]){
  source(file.path("Scripts","Matching","Helper","GenerateMatches.R"))
  source(file.path("Scripts","Matching","Helper","PerformMatch.R"))
  # pass all data to main function for matching
  ls.Matches   <- sapply(M_TYPE, function(m)
                                 sapply(row.names(df.Fail),
                                        function(i) GenerateMatches(df.Fail[i,],
                                                                    m,
                                                                    maxStringDist = opts$Match$maxStringDist[m],
                                                                    capCandPct    = opts$Match$capCandPct[m],
                                                                    capCandN      = opts$Match$capCandN[m],
                                                                    SAVE          = M_STORE,
                                                                    OutPath       = 'Data/Analysis',
                                                                    LoadFromFile  = c(PAR = FALSE, SEQ = TRUE)[M_STYLE],
                                                                    LoadPath      = 'Data/Analysis',
                                                                    VERBOSE       = VERB), 
                                        USE.NAMES = TRUE, simplify = FALSE),
                                        USE.NAMES = TRUE, simplify = FALSE)
  rm(GenerateMatches, PerformMatch)
}
```

Analyzing the number of candidate matches returned by each run allows me to assess the performance of the algorithm.
```{r analyzeNMatch, include = TRUE, warning=FALSE, message=FALSE}
  nMatches <- sapply(names(ls.Matches), function(m)
                                        sapply(names(ls.Matches[[m]]), function(i)
                                               ifelse(!is.na(as.integer(ls.Matches[[m]][[i]][3])),
                                                      -1*as.integer(ls.Matches[[m]][[i]][3]),
                                               sum(!grepl("IDENT",ls.Matches[[m]][[i]]))),
                                               USE.NAMES = TRUE, simplify = TRUE), 
                     USE.NAMES = TRUE, simplify = FALSE)
 nMatches$state   <- sapply(row.names(df.Fail), function(i) sum(df.NBI$STFIPS==df.Fail[i,"STFIPS"]))
 nMatches$county  <- sapply(row.names(df.Fail), function(i) sum(df.NBI$COUNTY_CODE_003==df.Fail[i,"FIPS_1"] |  
                                                        df.NBI$STFIPS==df.Fail[i,"FIPS_2"],na.rm = T))
 nMatches         <- as.data.frame(nMatches)
 nMatches$ID      <- row.names(nMatches)
 nMatches.melt    <- melt(nMatches, id.vars = "ID", variable.name = "Match", value.name = "Count")
 nMatches.melt$Match <- factor(nMatches.melt$Match, levels = c("state", "county",M_TYPE))
 nBar <- ggplot(nMatches.melt, aes(x = ID, y = Count, fill = Match)) + geom_col(position = position_dodge()) +
   geom_text(aes(label = Count, y = Count + 0.05), position = position_dodge(0.9), vjust = 0, size = sizesP$Reg["PRES"])
 nBar + scale_fill_manual(values = c(state = "gray65", county = "black", road = "red",stream = "blue", route = "orange",
                                     bin = "green")) + getTheme(outputType = "PRES") + 
   labs(title = "Number of candidate matches by match type", x = "Collapsed Bridge ID")+
   theme(axis.ticks.x = element_blank(), axis.ticks.y = element_blank())
```


# 5. Automated collating and ranking of potential matches
The match quality is automatically recorded by match type, which field of that type was matched, and whether the match was
exact, _____. In parallel mode, the matches can be combined across type to bring multi-matched candidates to the front.
```{r collateRanking, include=TRUE, eval=params$eval}
if (RUN["RANK"]){
  
  source(file.path("Scripts","Matching","Rank.Matched.Bridges.R"))

  ls.Matches.Comb <- Rank.Matched.Bridges(ls.Matches)
  
  rm(Rank.Matched.Bridges)
}
```

# 6. Supervised match confirmation
Shows a series of maps and data entry such that possible "true" matches can be identified.
```{r confirmation, echo=FALSE}
if (RUN["CONFIRM"]){
  ls.Comb <- list()
  MatchQual <- c(exact   = "e",
                 full    = "f",
                 partial = "p",
                 fuzzy   = "z")
  qualRank <- c(as.vector(sapply(1:3, function(i) paste0(i,MatchQual))),"")
 for(i in row.names(df.Fail)){
   dfF <- df.Fail[rep(i,length(ls.Matches.Comb[[i]]$MatchID)),1:11]
   dfF$OBJECTID_new <- ls.Matches.Comb[[i]]$MatchID
   dfF$Qual         <- ls.Matches.Comb[[i]]$Qual
   dfF$Q_BIN    <- sapply(row.names(dfF), 
                          function(i) ifelse(grepl("B",dfF[i,"Qual"]),
                                             substr(regmatches(dfF[i,"Qual"],regexpr("[1-3][efpz]B",dfF[i,"Qual"])),1,2),
                                             ""),
                          simplify = TRUE, USE.NAMES = FALSE)
   dfF$Q_STREAM <- sapply(row.names(dfF), 
                          function(i) ifelse(grepl("S",dfF[i,"Qual"]),
                                             substr(regmatches(dfF[i,"Qual"],regexpr("[1-3][efpz]S",dfF[i,"Qual"])),1,2),
                                             ""),
                          simplify = TRUE, USE.NAMES = FALSE)
   dfF$Q_ROAD   <- sapply(row.names(dfF), 
                          function(i) ifelse(grepl("R",dfF[i,"Qual"]),
                                             substr(regmatches(dfF[i,"Qual"],regexpr("[1-3][efpz]R",dfF[i,"Qual"])),1,2),
                                             ""),
                          simplify = TRUE, USE.NAMES = FALSE)
   dfF$Q_ROUTE  <- sapply(row.names(dfF), 
                          function(i) ifelse(grepl("T",dfF[i,"Qual"]),
                                             substr(regmatches(dfF[i,"Qual"],regexpr("[1-3][efpz]T",dfF[i,"Qual"])),1,2),
                                             ""),
                          simplify = TRUE, USE.NAMES = FALSE)
   Q_cols <- colnames(dfF)[grepl("Q_",colnames(dfF))]
   dfF[,Q_cols] <- sapply(Q_cols, function(j) factor(dfF[,j], levels = qualRank, labels = as.character(1:length(qualRank), drop = FALSE)))
   
   ls.Comb[[i]] <- merge(dfF, df.NBI[,c("OBJECTID_new","STATE_CODE_001",
                                "STRUCTURE_NUMBER_008","LOCATION_009",
                                "FACILITY_CARRIED_007","ROUTE_NUMBER_005D",
                                "FEATURES_DESC_006A","STRUCTURE_KIND_043A",
                                "STRUCTURE_TYPE_043B")], 
                         by = "OBJECTID_new")
   ls.Comb[[i]]$MAT_NBI <- factor(as.character(ls.Comb[[i]]$STRUCTURE_KIND_043A),
                                  levels = names(ls.ITEM43A.Ont),
                                  labels = unlist(lapply(ls.ITEM43A.Ont,"[[",1)))
   ls.Comb[[i]]$TYPE_NBI <- factor(as.character(ls.Comb[[i]]$STRUCTURE_TYPE_043B),
                                  levels = names(ls.ITEM43B.Ont),
                                  labels = unlist(lapply(ls.ITEM43B.Ont,"[[",1)))
   cols <- c("BIN","STRUCTURE_NUMBER_008","LOCATION","LOCATION_009",
                         "FACILITY_CARRIED_007","ROUTE_NUMBER_005D",
                         "FEAT_UND","FEATURES_DESC_006A","FAIL_TYPE","YR_BLT","YR_FAIL",
                         "MAT","MAT_NBI","TYPE","TYPE_NBI")
  #  formattable(ls.Comb[[i]][,cols],
  #              list(BIN = 
  # age = color_tile("white", "orange"),
  # grade = formatter("span", style = x ~ ifelse(x == "A", 
  #   style(color = "green", font.weight = "bold"), NA)),
  # area(col = c(test1_score, test2_score)) ~ normalize_bar("pink", 0.2),
  # final_score = formatter("span",
  #   style = x ~ style(color = ifelse(rank(-x) <= 3, "green", "gray")),
  #   x ~ sprintf("%.2f (rank: %02d)", x, rank(-x))),
  # registered = formatter("span",
  #   style = x ~ style(color = ifelse(x, "green", "red")),
  #   x ~ icontext(ifelse(x, "ok", "remove"), ifelse(x, "Yes", "No")))
# )))
   print(ls.Comb[[i]][,cols])
 }
}
```

# Notebook metadata
```{r sessionInfo, include=FALSE, echo=TRUE, results='markup'}
devtools::session_info()
```

# References

```{r cleanup, include=FALSE, echo=FALSE, error=FALSE, eval=params$eval}
# Hidden: clear temporary files and environment
if(M_STORE){
  DeleteList <- list.files(path = file.path("Analysis","Temp"), full.names = TRUE)
  file.remove(DeleteList)
}
rm(list = ls())
```

